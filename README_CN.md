# LiteLLM - Go å¤šå¹³å° LLM API å®¢æˆ·ç«¯

[English](README.md) | ä¸­æ–‡

ä¸€ä¸ªç®€æ´ä¼˜é›…çš„ Go è¯­è¨€åº“ï¼Œç”¨äºç»Ÿä¸€è®¿é—®å¤šä¸ª LLM å¹³å°ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **ç®€æ´æ˜“ç”¨** - ä¸€è¡Œä»£ç è°ƒç”¨ä»»æ„ LLM å¹³å°
- ğŸ”„ **ç»Ÿä¸€æ¥å£** - æ‰€æœ‰å¹³å°ä½¿ç”¨ç›¸åŒçš„è¯·æ±‚/å“åº”æ ¼å¼
- ğŸ§  **æ¨ç†æ”¯æŒ** - å®Œæ•´æ”¯æŒ OpenAI o ç³»åˆ—æ¨ç†æ¨¡å‹
- ğŸ› ï¸ **å·¥å…·è°ƒç”¨** - å®Œæ•´çš„ Function Calling æ”¯æŒ
- ğŸŒŠ **æµå¼å¤„ç†** - å®æ—¶æµå¼å“åº”
- ğŸ“¦ **é›¶é…ç½®** - ç¯å¢ƒå˜é‡è‡ªåŠ¨å‘ç°
- ğŸ”§ **æ˜“æ‰©å±•** - è½»æ¾æ·»åŠ æ–°çš„ LLM å¹³å°
- ğŸ¯ **ç±»å‹å®‰å…¨** - å¼ºç±»å‹å®šä¹‰å’Œå®Œå–„çš„é”™è¯¯å¤„ç†

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
go get github.com/voocel/litellm
```

### ä¸€è¡Œä»£ç ä½¿ç”¨

```go
package main

import (
    "fmt"
    "github.com/voocel/litellm"
)

func main() {
    // è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY="your-key"
    response, err := litellm.Quick("gpt-4o-mini", "ä½ å¥½ï¼ŒLiteLLMï¼")
    if err != nil {
        panic(err)
    }
    fmt.Println(response.Content)
}
```

### å®Œæ•´é…ç½®

```go
package main

import (
    "context"
    "fmt"
    "github.com/voocel/litellm"
)

func main() {
    // æ–¹å¼1: ç¯å¢ƒå˜é‡è‡ªåŠ¨å‘ç°
    client := litellm.New()
    
    // æ–¹å¼2: æ‰‹åŠ¨é…ç½® (ç”Ÿäº§ç¯å¢ƒæ¨è)
    client = litellm.New(
		litellm.WithOpenAI("your-openai-key"),
		litellm.WithAnthropic("your-anthropic-key"),
		litellm.WithGemini("your-gemini-key"),
		litellm.WithDefaults(2048, 0.8), // è‡ªå®šä¹‰é»˜è®¤å‚æ•°
    )
    
    // åŸºç¡€èŠå¤©
    response, err := client.Complete(context.Background(), &litellm.Request{
        Model: "gpt-4o-mini",
        Messages: []litellm.Message{
            {Role: "user", Content: "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"},
        },
        MaxTokens:   litellm.IntPtr(200),
        Temperature: litellm.Float64Ptr(0.7),
    })
    
    if err != nil {
        panic(err)
    }
    
    fmt.Printf("å›ç­”: %s\n", response.Content)
    fmt.Printf("Tokens: %d (è¾“å…¥: %d, è¾“å‡º: %d)\n", 
        response.Usage.TotalTokens, 
        response.Usage.PromptTokens, 
        response.Usage.CompletionTokens)
}
```

## ğŸ§  æ¨ç†æ¨¡å‹

å®Œæ•´æ”¯æŒ OpenAI o ç³»åˆ—æ¨ç†æ¨¡å‹ï¼ŒåŒ…æ‹¬ Chat API å’Œ Responses APIï¼š

```go
response, err := client.Complete(context.Background(), &litellm.Request{
    Model: "o3-mini",
    Messages: []litellm.Message{
        {Role: "user", Content: "é€æ­¥è®¡ç®— 15 * 8"},
    },
    MaxTokens:        litellm.IntPtr(500),
    ReasoningEffort:  "medium",      // "low", "medium", "high"
    ReasoningSummary: "detailed",    // "concise", "detailed", "auto"
    UseResponsesAPI:  true,          // å¼ºåˆ¶ä½¿ç”¨ Responses API
})

// è·å–æ¨ç†è¿‡ç¨‹
if response.Reasoning != nil {
    fmt.Printf("æ¨ç†è¿‡ç¨‹: %s\n", response.Reasoning.Summary)
    fmt.Printf("æ¨ç† tokens: %d\n", response.Reasoning.TokensUsed)
}
```

## ğŸŒŠ æµå¼å¤„ç†

æ”¯æŒå®æ—¶æµå¼å“åº”å’Œæ¨ç†è¿‡ç¨‹å±•ç¤ºï¼š

```go
stream, err := client.Stream(context.Background(), &litellm.Request{
    Model: "gpt-4o-mini",
    Messages: []litellm.Message{
        {Role: "user", Content: "è®²ä¸€ä¸ªç¼–ç¨‹ç¬‘è¯"},
    },
})

defer stream.Close()
for {
    chunk, err := stream.Read()
    if err != nil || chunk.Done {
        break
    }
    
    switch chunk.Type {
    case litellm.ChunkTypeContent:
        fmt.Print(chunk.Content)
    case litellm.ChunkTypeReasoning:
        fmt.Printf("[æ€è€ƒ: %s]", chunk.Reasoning.Summary)
    }
}
```

## ğŸ› ï¸ å·¥å…·è°ƒç”¨ (Function Calling)

å®Œæ•´æ”¯æŒ Function Callingï¼Œå…¼å®¹ OpenAI å’Œ Anthropicï¼š

```go
tools := []litellm.Tool{
    {
        Type: "function",
        Function: litellm.FunctionSchema{
            Name:        "get_weather",
            Description: "è·å–åŸå¸‚å¤©æ°”ä¿¡æ¯",
            Parameters: map[string]interface{}{
                "type": "object",
                "properties": map[string]interface{}{
                    "city": map[string]interface{}{
                        "type":        "string",
                        "description": "åŸå¸‚åç§°",
                    },
                },
                "required": []string{"city"},
            },
        },
    },
}

response, err := client.Complete(context.Background(), &litellm.Request{
    Model: "gpt-4o-mini",
    Messages: []litellm.Message{
        {Role: "user", Content: "åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
    },
    Tools:      tools,
    ToolChoice: "auto",
})

// å¤„ç†å·¥å…·è°ƒç”¨
if len(response.ToolCalls) > 0 {
    // æ‰§è¡Œå‡½æ•°å¹¶ç»§ç»­å¯¹è¯...
}
```

## ğŸ”§ æ‰©å±•æ–°å¹³å°

æ·»åŠ æ–°çš„ LLM å¹³å°éå¸¸ç®€å•ï¼š

```go
// å®ç° Provider æ¥å£
type MyProvider struct {
    *litellm.BaseProvider
}

func (p *MyProvider) Complete(ctx context.Context, req *litellm.Request) (*litellm.Response, error) {
    // å®ç° API è°ƒç”¨é€»è¾‘
    return &litellm.Response{
        Content:  "æ¥è‡ªæˆ‘çš„ provider çš„é—®å€™ï¼",
        Model:    req.Model,
        Provider: "myprovider",
        Usage:    litellm.Usage{TotalTokens: 10},
    }, nil
}

// æ³¨å†Œ provider
func init() {
    litellm.RegisterProvider("myprovider", NewMyProvider)
}

// ä½¿ç”¨
client := litellm.New()
response, _ := client.Complete(ctx, &litellm.Request{
    Model: "my-model",
    Messages: []litellm.Message{{Role: "user", Content: "ä½ å¥½"}},
})
```

## ğŸ“‹ æ”¯æŒçš„å¹³å°

### OpenAI
- âœ… GPT-4o, GPT-4o-mini, GPT-4.1, GPT-4.1-mini, GPT-4.1-mano
- âœ… o3, o3-mini, o4-mini (æ¨ç†æ¨¡å‹)
- âœ… Chat Completions API å’Œ Responses API
- âœ… Function Calling, Vision, æµå¼å¤„ç†

### Anthropic
- âœ… Claude 3.7 Sonnet, Claude 4 Sonnet, Claude 4 Opus
- âœ… Function Calling, Vision, æµå¼å¤„ç†

### Google Gemini
- âœ… Gemini 2.5 Pro, Gemini 2.5 Flash
- âœ… Function Calling, Vision, æµå¼å¤„ç†
- âœ… è¶…å¤§ä¸Šä¸‹æ–‡çª—å£

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡
```bash
export OPENAI_API_KEY="sk-proj-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="AIza..."
```

### ä»£ç é…ç½® (æ¨è)
```go
client := litellm.New(
    litellm.WithOpenAI("your-openai-key"),
    litellm.WithAnthropic("your-anthropic-key"),
    litellm.WithGemini("your-gemini-key"),
    litellm.WithDefaults(2048, 0.8),
)
```

## ğŸ“Š API å‚è€ƒ

### æ ¸å¿ƒç±»å‹
```go
type Request struct {
    Model            string    `json:"model"`                 // æ¨¡å‹åç§°
    Messages         []Message `json:"messages"`              // å¯¹è¯æ¶ˆæ¯
    MaxTokens        *int      `json:"max_tokens,omitempty"`  // æœ€å¤§tokenæ•°
    Temperature      *float64  `json:"temperature,omitempty"` // é‡‡æ ·æ¸©åº¦
    Tools            []Tool    `json:"tools,omitempty"`       // å¯ç”¨å·¥å…·
    ReasoningEffort  string    `json:"reasoning_effort,omitempty"`  // æ¨ç†å¼ºåº¦
    ReasoningSummary string    `json:"reasoning_summary,omitempty"` // æ¨ç†æ‘˜è¦
}

type Response struct {
    Content   string         `json:"content"`              // ç”Ÿæˆå†…å®¹
    ToolCalls []ToolCall     `json:"tool_calls,omitempty"` // å·¥å…·è°ƒç”¨
    Usage     Usage          `json:"usage"`                // Tokenä½¿ç”¨ç»Ÿè®¡
    Reasoning *ReasoningData `json:"reasoning,omitempty"`  // æ¨ç†æ•°æ®
}
```

### ä¸»è¦æ–¹æ³•
```go
func Quick(model, message string) (*Response, error)
func New(opts ...ClientOption) *Client
func (c *Client) Complete(ctx context.Context, req *Request) (*Response, error)
func (c *Client) Stream(ctx context.Context, req *Request) (StreamReader, error)
```

## ğŸ“„ è®¸å¯è¯

Apache License

---

**LiteLLM** - è®© LLM API è°ƒç”¨å˜å¾—ç®€å•ä¼˜é›… ğŸš€